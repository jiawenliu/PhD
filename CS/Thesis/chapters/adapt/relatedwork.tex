\section{The Dependency Semantics Analysis}
\label{sec:relatedwork-adapt-semantics}
My framework constructs an execution-based dependency graph based on the execution traces of a program. 
    I define semantic dependence on this graph by considering (intraprocedural) data and control 
    dependency~\cite{bilardi1996framework,cytron1991efficiently,pollock1989incremental}.    
    One related work  
    \cite{austin1992dynamic} presents a methodology to construct a dynamic dependency graph (DDG) based on the dynamic execution of a program in an imperative language, where edges represent dependency between instructions. Data dependency, control dependency, storage dependency, and resource dependency between instructions are all considered. My execution-based dependency graph only needs data dependency and control dependency between variable assignment results. 
    % Critical path length analysis on DDGs is useful for understanding the scope for parallelization, while we use the length of the longest path to define adaptivity.  
    %
    DDGs have been used in many other domains. \cite{nagar2018automated} use DDGs to find serializability violations. \cite{hammer2006dynamic} use similar \emph{program dependency graphs} \cite{ferrante1987program} for dynamic program slicing.
    \cite{mastroeni2008data} propose ways of constructing different kinds of program slices, by choose different program dependency. 

    They actually use a combination of  
    static and dynamic dependency graphs but in a manner that is different from how we use the two. Their slicing uses both static and dynamic dependency graphs, while we use the dynamic dependency graph as the basis of a definition, which is then soundly approximated by an analysis based on the static dependency graph.-
    
    My execution-based data dependency relation definition over variables 
    is inspired by the method in \cite{Cousot19a}, where the dependency relation is also identified by looking into the differences on two execution traces. 
    However, Cousot excludes timing channels~\cite{SabelfeldM03} and empty observation, which are also not considered as a form of dependency in traditional dependency analysis \cite{DenningD77}.
    % In the cases of empty observation and timing channels, the second query is executed 
    % in one trace and isn't in another trace by modifying value of first query. 
    % Then, the second query is indeed depend on the first query and there exists an
    % adaptivity round between the two queries. 
    My definition includes timing channels and empty observation by observing both the disappearance and value variation.
  

\section{The Static Program Analysis}
\label{sec:relatedwork-adapt-static}
My design of $\THESYSTEM$ in this section is influenced by many areas of static program analysis such as 
effect systems, control-flow analysis, and data-flow analysis~\cite{ryder1988incremental}. 
The idea of statically estimating a sound upper bound for the adaptivity from the semantics is indirectly inspired from prior work on cost analysis via effect systems~\cite{cciccek2017relational,radivcek2017monadic,qu2019relational}. The idea of defining adaptivity using data flow is inspired by the work of graded 
Hoare logic~\cite{gaboardi2021graded}, which reasons about data flows as a resource. 
%
One of the most important ingredients of my work is the estimation of the program-based dependency graph. 
There are many ways to construct a dependency graph statically.
Some of the most related work focuses on the testing of graphical user interfaces (GUIs), using an event graph. For example, \cite{memon2007event} proposes an event-flow model using an algorithm to construct an event-flow graph, representing all the possible event interactions. 
This event-flow graph has a vertex for every GUI event such as click-to-paste and an edge between pairs of events that can be performed immediately one after the other. My program-based dependency graph uses the edge to track the may-dependence of one variable with respect to another variable. 
The main difference is in the way the graph is constructed. {\THESYSTEM} relies on the structure of the target program, while the event-flow model only considers the event type. 
Another work \cite{arlt2012lightweight} constructs a weighted event-dependency graph, capturing data dependencies between events by analyzing bytecode. 
Every weighted edge indicates a dependency between two events, meaning one event possibly reads data written by the other event, with the weight showing the intensity of the dependency (the quantity of data involved). 
My approach of generating the program-based dependency graph shares the idea of tracking data dependency via static analysis on the smyce code. 
However, because of the different domains, we care about assigned variables, and we use the weight differently to find a finite walk in the graph.

Moreover, the state-of-art data-flow analysis techniques do not
consider the quantitative information on how many times each variable is dependent on the other. 
My weight estimation is inspired by
% (specifically in the case if
% the data-flow is nested in iterations of recursion into consideration). 
 works in program complexity analysis and worst case execution time analysis areas, 
 focusing on analyzing the cost of the entire program. 
The techniques are based on
type system~\cite{CicekBG0H17, RajaniG0021}, Hoare logic~\cite{CarbonneauxHS15}, abstract interpretation~\cite{GustafssonEL05, HumenbergerJK18},
invariant generation through cost equations or ranking functions~\cite{BrockschmidtEFFG16,AlbertAGP08,AliasDFG10,Flores-MontoyaH14}
or a combination of program abstraction and invariant inferring~\cite{GulwaniZ10, SinnZV17,GulwaniJK09}.
In general, these techniques give the approximated upper bound of the program's total running time or resource cost.
However, they failed to consider the case where the cost -- the adaptivity-- could decrease when there isn't a dependency relation between variables.

\section{Generalization in Adaptive Data Analysis}
\label{sec:relatedwork-adapt-error}
Starting from the works by \citet{DworkFHPRR15} and \citet{HardtU14}, several works have designed methods that ensure generalization for adaptive data analyses.
Some examples
are:~\cite{dwork2015reusable,dwork2015generalization,
BassilyNSSSU16,UllmanSNSS18,FeldmanS17,jung2019new,SteinkeZ20,RogersRSSTW20}.
Several of these works drew inspiration from the idea of using methods designed to ensure differential privacy, a notion of formal data privacy, in order to guarantee generalization for adaptive data analyses.
By limiting the influence that an individual can have on the result of a data analysis, even in adaptive settings, differential privacy can also be used to limit the influence that a specific data sample can have on the statistical validity of a data analysis.
This connection is actually in two directions, as discussed for example by \citet{YeomGFJ18}.

Considering this connection between generalization and privacy, it is not surprising
that some works on programming language techniques for privacy-preserving data analysis are related to our work. 
Adaptive Fuzz~\cite{Winograd-CortHR17} is a programming framework for differential privacy that is designed around the concept of adaptivity. 
This framework is based on a typed functional language that distinguish between several forms of adaptive and non-adaptive composition theorem with the goal of achieving better upper bounds on the privacy cost.
Adaptive Fuzz uses a type system and some partial evaluation to guarantee that the programs respect differential privacy.
However, it does not include any technique to bound the number of rounds of adaptivity. 
\citet{lobo2021programming} propose a language for differential privacy where one can reason about the accuracy of programs in terms of confidence intervals on the error that the use of differential privacy can generate. These are akin to bounds on the generalization error. This language is based on a static analysis which however cannot handle adaptivity. 
%
The way we formalize the access to the data mediated by a mechanism is a reminiscence of how the interaction with an oracle is modeled in the verification of security properties. As an example, the recent works by \citet{BarbosaBGKS21} and \citet{AguirreBGGKS21} use different techniques to track the number of accesses to an oracle. However, reasoning about the number of accesses is easier than estimating the adaptivity of these calls, as we do instead here.
