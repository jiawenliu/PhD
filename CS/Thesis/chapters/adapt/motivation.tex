\subsection{Some results in Adaptive Data Analysis}
%\wq{I think we can move this subsection into appendix. Maybe just leave theorm 1.2 and 1.3}
%\jl{I don't agree}
In Adaptive Data Analysis an \emph{analyst} is interested in studying some distribution $\dist$ over some domain $\univ$.  Following previous works~\cite{DworkFHPRR15,HardtU14,BassilyNSSSU16}, we focus on the setting where the analyst is interested in answers to \emph{statistical queries} (also known as \emph{linear queries}) over the distribution.  A statistical query is usually defined by some function $\query \from \univ \to [-1,1]$ (often other codomains such as $[0,1]$ or $[-R,+R]$, for some $R$, are considered).  The analyst wants to learn the \emph{population mean}, which (abusing notation) is defined as 
$\query(\dist) = {\sample \sim \dist}{\query(\sample)}$. 
%
We assume that the distribution $\dist$ can only be accessed via a set of \emph{samples} $\sample_1,\dots,\sample_n$ drawn independently and identically distributed (i.i.d.)from $\dist$.  These samples are held by a mechanism $\mech(\sample_1,\dots,\sample_n)$ who receives the query $\query$ and computes an answer 
$\answer \approx \query(\dist)$.
%
The na\"ive way to approximate the population mean is to use the \emph{empirical mean}, which (abusing notation) is defined as 
$\query(\sample_1,\dots,\sample_n) = \frac{1}{n} \sum_{i=1}^{n} \query(X_i)$.
However, the mechanism $M$ can adopt some methods for improving the generalization error $| a- \query(\dist)|$.

In this work we consider analysts that ask a sequence of $k$ queries $\query_1,\dots,\query_k$.  If the queries are all chosen in advance, independently of the answers of each other, then we say they are \emph{non-adaptive}.  If the choice of each query $\query_j$ depend on the prefix $\query_1,\answer_1,\dots,\query_{j-1},\answer_{j-1}$ then they are \emph{fully adaptive}.  An important intermediate notion is \emph{$\qrounds$-round adaptive}, where the sequence can be partitioned into $\qrounds$ batches of non-adaptive queries.  Note that non-adaptive queries are $1$-round and fully adaptive queries are $k$-round adaptive.

We now review what is known about the problem of answering $r$-round adaptive queries.  
\begin{thm}[\cite{BassilyNSSSU16}] 
\label{thm:nonadapt-adapt}
\begin{enumerate}

\item For any distribution $\dist$, and any $k$ \emph{non-adaptive} statistical queries, 
% $$
$
\max_{j=1,\dots,k} | \answer_j - \query_j(\dist) | = O\left( \sqrt{\frac{\log k}{n}}  \right)
% $$
$.
%
\item For any distribution $\dist$, and  any $k$  \emph{$\qrounds$-round adaptive} statistical queries, with $\qrounds \geq 2$, the empirical mean (rounded to an appropriate number of bits of precision)\footnote{With infinite precision even two queries may give unbounded error, when the first query's result encodes the whole data.} satisfies:\\
% $$
$
\max_{j=1,\dots,k} | \answer_j - \query_j(\dist) | = O\left( \sqrt{\frac{k}{n}}  \right)
% $$
$
\end{enumerate}
\end{thm}
In fact, these bounds are tight (up to constant factors) which means that even allowing one extra round of adaptivity leads to an exponential increase in the generalization error, from $\log k$ to $k$.

\cite{DworkFHPRR15} and \cite{BassilyNSSSU16} showed that by using carefully calibrated Gaussian noise in order to limit the dependency of a single query on the specific data instance, one 
can actually achieve much stronger generalization error as a function of the number of queries, specifically.
\begin{thm}[\cite{DworkFHPRR15, BassilyNSSSU16}] \label{thm:gaussiannoise} For any distribution $\dist$, any $k$, any $\qrounds \geq 2$ and any \emph{$\qrounds$-round adaptive} statistical queries, if we answer queries with carefully calibrated Gaussian noise we have:
\begin{center}
  $
\max_{j=1,\dots,k} | \answer_j - \query_j(\dist) | = O\left( \frac{\sqrt[4]{k}}{\sqrt{n}}  \right)
$  
\end{center}
\end{thm}
% Notice that in order to Theorem~\ref{thm:gaussiannoise} has different quantification in that the optimal choice of mechanism depends on the number of queries.  Thus, we need to know the number of queries \emph{a priori} to choose the best mechanism.
More interestingly, \cite{DworkFHPRR15}
also gave a refined bounds that can be achieved with different mechanisms depending on the number of rounds of adaptivity.   \begin{thm}[\cite{DworkFHPRR15}] \label{thm:gaussiannoise2} For any $r$ and $k$, there exists a mechanism such that for any distribution $\dist$, and any $\qrounds \geq 2$ any \emph{$\qrounds$-round adaptive} statistical queries, it satisfies
\begin{center}
  $
\max_{j=1,\dots,k} | \answer_j - \query_j(\dist) | = O\left( \frac{r \sqrt{\log k}}{\sqrt{n}}  \right)
$  
\end{center}
\end{thm}
Notice that Theorem~\ref{thm:gaussiannoise2} has different quantification in that the optimal choice of mechanism depends on the number of queries {and number of rounds of adaptivity}.  This suggests that if one knows a good \emph{a priori upper bound on the number of rounds of adaptivity}, one can choose the appropriate mechanism and get a much better guarantee in terms of generalization error.
As an example, as we can see in Fig.~\ref{fig:generalization_errors}, if we know that an algorithm is two rounds adaptive, we can choose data splitting as {the} mechanism, while if we know that an algorithm has many rounds of adaptivity we can choose Gaussian noise. It is worth to stress that by knowing the number of rounds of adaptivity one can also compute a concrete upper bound on the generalization error of a data analysis. This information allows one to have a quantitative, a priori, estimation of the effectiveness of a data analysis. 
This motivates us to design a static program analysis aimed at giving good \emph{a priori} upper bounds on the number of rounds of adaptivity of a program. 


\subsection{Challenges of Analyzing Adaptivity}
\label{sec:adapt-intro-challenge}
Given the significance of this \emph{adaptivity} quantity in the data analysis area,
I'm motivated to analyze this property.
In order to analyze this property, there are mainly three challenges introduced in Section~\ref{sec:adapt-intro-challenge}.
Then targeting the three challenges, I give a technical overview of the new adaptivity analysis framework
with respect to the limitations of previous works, in Section~\ref{sec:adapt-intro-overview}.

There are mainly three challenges in order to analyze this adaptivity property, 
and the full-spectrum analysis of this property is 
% In this proposal, I will first focus on analyzing 
% this adaptivity property for the program based on solving 
developed w.r.t. the three challenges accordingly.

\begin{enumerate}
 \item
 \textbf{Adaptive Data Analysis Formalization}

The first challenge is \emph{how to define formally} a model for adaptive data analysis which is general enough to support the methods I discussed above and would permit to the formulation of the notion of adaptivity these methods use. 
I take the approach of designing a programming framework for submitting queries to some \emph{mechanism} giving access 
to the data mediated by one of the techniques which are mentioned before, 
including the mechanism of adding Gaussian noise, 
the mechanism that randomly selects a subset of the data, 
and the mechanism that uses the reusable holdout technique, etc. 
In this approach, a program models an \emph{analyst} asking a sequence of queries to the mechanism. 
The mechanism runs the queries on the data applying one of the methods discussed above and returns the result to the program. The program can then use this result to decide which query to run next. 
% Overall, I am interested in controlling the generalization of the results of the queries which are returned by the mechanism, by means of adaptivity. 

% \textbf{Methodology}
% There are previous works from \cite{weihao22} developing language formalizing the adaptive data analysis.
% However, their formalization is limited in the expressiveness largely.
Motivated by this, I present a new while-like language 
named {\tt Query While} language with extensions on query requests in Section~\ref{sec:adapt-language}.

\item 
\textbf{Adaptivity Formalization}

The second challenge is \emph{how to define the adaptivity of a given program}.
Intuitively, a query $Q$ may depend on another query $P$, if there are two values that $P$ can return which affect in different ways the execution of $Q$. 
For example, as shown in \cite{dwork2015reusable}, and as I did in our example in Figure~\ref{fig:generalization_errors}(a), one can design a machine learning algorithm for constructing a classifier that first computes each feature's correlations with the label via a sequence of queries, and then constructs the classifier based on the correlation values. 
If one feature's correlation changes, the classifier depending on features is also affected. 
This notion of dependency builds on the execution trace as a \emph{causal history}. 
In particular, I am interested in the history or provenance of a query up until this is executed, 
% I am not then concerned about how the result is used --- except for 
simultaneously in tracking whether the result of the query may further cause some other query. 
This is because I'm focusing on the generalization error which could be propagated by queries.
% and not their post-processing. % 

To formalize this intuitive \emph{adaptivity} as a quantitative program property, 
I analyze the program's execution based on its semantics
%  develop an execution-based analysis 
in Section~\ref{sec:adapt-exe}.
\item 
\textbf{Adaptivity Estimation}

The third challenge is \emph{how to estimate the adaptivity of a given program}. 
The adaptive data analysis model I consider and our definition of adaptivity suggest that for this task 
I can use a program analysis that is based on some form of dependency analysis.
 This analysis needs to take into consideration:
1) the fact that, in general, a query $Q$ is not a monolithic block but rather it may depend, through the use of variables and values, on other parts of the program. 
Hence, it needs to consider some form of data flow analysis. 
2) the fact that, in general, the decision on whether to run a query or not may depend on some other value. Hence, 
 it needs to consider some form of control flow analysis.
3) the fact that. in general, I am not only interested in whether there is a dependency or not, but in the length of the chain of dependencies. 
Hence, it needs to consider some quantitative information about the program dependencies. % {A quick example is that: I store the result of query $Q_1$ in variable $x$ and use variable $y$ to record the result of query $Q_2$. I want to construct the third query $Q_3$ which relies on the value stored in $x$, let us say, $Q_3$ will ask for the sum of the first column of a table if $x$ is positive and the sum of the second column otherwise. In this situation, I need data flow analysis. On the other hand, if I need the value of $y$ to help us decide whether I should ask $Q_3$, for example, I ask the third query if $y$ is odd, and do not ask if $y$ is even. Naturally, to be able to handle this case, control flow analysis comes into play. Formally speaking, }

To address these considerations and be able to estimate a sound upper bound on the adaptivity of a program, 
I develop a static program analysis in Section~\ref{sec:adapt-static}, named {\THESYSTEM}.
%%%%% To reason about%
\end{enumerate}
% \\
%
\subsection{Technique Overview}
\label{sec:adapt-intro-overview}

\subsubsection{Language Design}
\paragraph*{Research Goal - Adaptive Data Analysis Formalization}
Targeting the first challenge
% --\textbf{Adaptivity Formalization}--
for analyzing the adaptive data analysis, 
the research goal is to develop
an expressive language supporting general adaptive data analysis formally.
% analysis method which can
% % The second challenge is 
% \emph{define} the intuitive \emph{adaptivity} rounds for a given data analysis program formally and accurately.

\paragraph*{Limitations of Previous Works}
There are previous works from \cite{weihao22} developing language formalizing the adaptive data analysis.
However, these language designs are limited in their expressiveness largely.
Most of the general data analyses are not supported by the previous language design.
In Section~\ref{sec:prework-language}, I introduce in detail the previous language design and limitations.
%
\paragraph*{Methodology Overview}
I design a new while-like language 
named {\tt Query While} language in Section~\ref{sec:adapt-language}.
This language is extended from the standard language.
It supports more general data analysis and query requests with more expressive expressions than previous works.

\subsubsection{Adaptivity Formalization}
\label{sec:intro-exe}
% \begin{enumerate}
% \item
% \textbf{Adaptive Data Analysis Formalization}
% The first challenge is \emph{how to define formally} a model for adaptive data analysis which is general enough to support the methods I discussed above and would permit to formulate the notion of adaptivity these methods use. 
% I take the approach of designing a programming framework for submitting queries to some \emph{mechanism} giving access to the data mediated by one of the techniques I mentioned before, e.g., adding Gaussian noise, randomly selecting a subset of the data, using the reusable holdout technique, etc. 
% In this approach, a program models an \emph{analyst} asking a sequence of queries to the mechanism. The mechanism runs the queries on the data applying one of the methods discussed above and returns the result to the program. The program can then use this result to decide which query to run next. 
% % Overall, I'm interested in controlling the generalization of the results of the queries which are returned by the mechanism, by means of adaptivity. 
% \item 
\paragraph{Research Goal - Define the Adaptivity formally}
Targeting the second challenge
% --\textbf{Adaptivity Formalization}--
for analyzing the adaptivity, 
the research goal is to develop an analysis method which can
% The second challenge is 
\emph{define} the intuitive \emph{adaptivity} rounds for a given data analysis program formally and accurately.
% Intuitively, a query $Q$ may depend on another query $P$, if there are two values that $P$ can return which affect in different ways the execution of $Q$. 
% For example, as shown in \cite{dwork2015reusable}, and as I did in our example in Figure~\ref{fig:generalization_errors}(a), one can design a machine learning algorithm for constructing a classifier that first computes each feature's correlations with the label via a sequence of queries, and then constructs the classifier based on the correlation values. 
% If one feature's correlation changes, the classifier depending on features is also affected. 
The intuition behind this \emph{adaptivity} quantity relies on the dependency between the query requests 
executed in the program, and their dependency depth. 
% This notion of dependency builds on the execution trace as a \emph{causal history}. 
Following this intuition, we need to formally define whether there is the dependency between query requests during the program
execution.
%
% And the 
There are different methodologies that can be used to formalize the dependency between two query requests, such as the 
dynamic analysis, execution-based analysis, static analysis, etc.
%
In order to capture the dependency (also the intuitive \emph{adaptivity}) in the most precise way,
% I design the execution-based program analysis method.
I formalize the adaptivity based on the program's execution.
%
% In order to define it in 
% The most precise way to define this dependency is by observing the actual evaluation of these query requests during the 
% program execution.
This execution-based
% program analysis method 
formalization is precise in defining both the dependency relation
% between query requests,
and the intuitive \emph{adaptivity} because it is
% In particular, I'm interested in the history or provenance of a query up until this is executed, I'm not then concerned about how the result is used --- except for tracking whether the result of the query may further cause some other query. 
% Based on this, I design the adaptivity formalization method through the execution-based program analysis techniques.
% These execution-based analysis techniques are 
based on observing the actual evaluation of these query requests during the 
program execution. This is consistent with the intuition of query requests dependency and their dependency depth,
as well as the intuitive
\emph{adaptivity}.
% This is because I focus on the generalization error of queries and not their post-processing. % 
\paragraph{Limitations of Existing Program Semantics and Execution Analysis Techniques}

\begin{itemize}
 \item \textbf{Limitations in Program Semantics and Execution Analysis Area}
 \\
There are many techniques for analyzing a program's semantics or execution, such are program simulation, 
semantics analysis, abstract interpretation, etc., which are widely used in defining
and formalizing the program's properties.
% dependency analysis. 
However, it is not straightforward how to apply them to analyzing and formalizing the \emph{adaptivity}
for adaptive data analysis program.
There are following three limitations among existing execution-based analysis techniques.
\begin{enumerate}
\item The state-of-art dependency relation analysis techniques don't
consider both control influence and value influence. However, both of these
two influences are included in the intuitive query dependency 
in adaptive data analysis programs.
% require
\item In the existing data dependency analysis area, there isn't any technique taking into account
the dependency depth. However, this dependency depth is the key quantity in formalizing the intuitive \emph{adaptivity}.
\item The intuitive \emph{adaptivity} doesn't accumulate linearly through the dependency relation and 
dependency depth. There is only very limited works related to this non-linear quantity property analysis.
This requires us to design new analysis techniques in order to formalize this quantity precisely.
% here isn't any research combining the two pieces of information. No need to mention the adaptivity analysis.
\end{enumerate}
\item \textbf{Limitations in Previous Works on Formalizing Adaptivity} 
\\
There are also previous works on formalizing this quantity from \cite{weihao22}. However, these works
are limited in expressiveness and accuracy significantly.
The \emph{adaptivity} quantities in many general data analysis programs are unable to be defined in the previous \emph{adaptivity} definition.
For the program which is able to be defined,
they cannot give the most precise adaptivity definitions matching the intuitive \emph{adaptivity}.
These previous works and limitations on formalizing the adaptivity are introduced in more detail in Section~\ref{sec:prework-formalization}.
\end{itemize}
%
\paragraph{Methodology Overview}
% To formalize this intuition as a quantitative program property, 
% in Chapter~\ref{ch:dynamic} I develop an execution-based analysis
% % I first consider all the possible evaluations of a program --- I do this by 
% % I use trace semantics recording the execution history of programs on some given input --- and I create a dependency graph, where the dependency between different variables (query is also assigned to a variable) is explicit and track which variable is associated with a query request. 
% % I then enrich this graph with weights describing the maximal number of times each variable is evaluated in a program evaluation starting with an initial state. The adaptivity is then defined as the length of the walk visiting most query-related variables on this graph. 
% % Through two aspects: the execution-based analysis and static-based program analysis.
% % In the execution-based analysis, I will formalize the intuitive notion of \emph{adaptivity} as a quantitative 
% % property of programs. This analysis is developed 
% in three steps through different methodologies in each step as follows,
Given all these limitations and challenges,
to define this intuitive \emph{adaptivity} more precisely as a quantitative program property, 
I give a new adaptivity formalization model by analyzing program's exectuion in Section~\ref{sec:adapt-exe}.
% I first consider all the possible evaluations of a program --- I do this by 
% I use a trace semantics recording the execution history of programs on some given input --- and I create a dependency graph, where the dependency between different variables (query is also assigned to a variable) is explicit and track which variable is associated with a query request. 
% I then enrich this graph with weights describing the maximal number of times each variable is evaluated in a program evaluation starting with an initial state. The adaptivity is then defined as the length of the walk visiting most query-related variables on this graph. 
% Through two aspects: the execution-based analysis and static-based program analysis.
% In the execution-based analysis, I will formalize the intuitive notion of \emph{adaptivity} as a quantitative 
% property of programs. This analysis is developed 
 This execution-based formalization is designed in three steps through different methodologies as follows,
% \begin{enumerate}
% \item The dependency relation between every query, through the methodology of semantic data dependency analysis.
% \\
% Specifically through a trace semantics recording the execution history of programs on some given input
% % --- and I create a dependency graph, 
% the dependency between different variables (query is also assigned to a variable) is explicit and track which variable is associated with a query request. 
% % I then enrich this graph with weights describing the maximal number of times each variable is evaluated in a program evaluation starting with an initial state. The adaptivity is then defined as the length of the walk visiting most query-related variables on this graph. 
% % In the execution-based analysis, I will formalize the intuitive notion of \emph{adaptivity} as a quantitative 
% % property of programs. This analysis is developed 
% % \\
% \item The dependency quantity analysis, through the methodology of execution-based data reachability, bound analysis.
% % \\
% \item The adaptivity quantity analysis, based on the two analysis results above, gives the formal \emph{adaptivity} model 
% for program.
% \\
% Specifically, I create a dependency graph, where the dependency between different variables (query is also assigned to a variable) is explicit and track which variable is associated with a query request. 
% I then enrich this graph with weights describing the maximal number of times each variable is evaluated in a program evaluation starting with an initial state. 
% The adaptivity is then defined as the length of the walk visiting most query-related variables on this graph. 
% \end{enumerate}
\begin{enumerate}
 \item First step is to define the \emph{dependency relation} between every query, 
 through the methodology of semantic data dependency analysis.
 %
 Specifically through a trace semantics recording the execution history of programs on a given input,
 % --- and I create a dependency graph, 
 the dependency between different variables (query is also assigned to a variable) is explicitly tracked and 
 analyzed. This is presented in Section~\ref{sec:dynamic-datadep}.
 % and 
 % which variable is associated with a query request. 
 % I then enrich this graph with weights describing the maximal number of times each variable is evaluated in a program evaluation starting with an initial state. The adaptivity is then defined as the length of the walk visiting most query-related variables on this graph. 
 % In the execution-based analysis, I will formalize the intuitive notion of \emph{adaptivity} as a quantitative 
 % property of programs. This analysis is developed 
 % \\
 \item The second step in Section~\ref{sec:dynamic-reachability} is to analyze the \emph{dependency quantity} 
 % analysis, 
 based on the \emph{dependency relation} above.
 This analysis is developed through the methodology of execution-based reachability bound analysis.
 % \\
 % \item The last step in Section~\ref{sec:dynamic-adapt} is the intuitive \emph{adaptivity} quantity analysis, 
 % according to the two analysis results above, specifically \emph{dependency relation} and \emph{dependency quantity}.
 % This analysis is developed through the formal \emph{adaptivity} definition. \\
 % Specifically, I create a dependency graph, where the dependency between different variables (query is also assigned to a variable) is explicit and track which variable is associated with a query request. 
 % I then enrich this graph with weights describing the maximal number of times each variable is evaluated in a program evaluation starting with an initial state. 
 % The adaptivity is then defined as the length of the walk visiting most query-related variables on this graph. 
 \item The last step is the intuitive \emph{adaptivity} quantity definition, 
 according to the two analysis results above, specifically \emph{dependency relation} and \emph{dependency quantity}.
 This step 
 % is developed through 
 gives the formal \emph{adaptivity} definition. 
 \\
 Specifically, this analysis is developed by creating a dependency graph first. 
 In this graph, the dependency between different variables (query is also assigned to a variable) 
 is explicit and tracks which variable is associated with a query request. 
 This dependency comes from the \emph{dependency relation} from the first step analysis.
 \\
 Then, I enrich this graph with 
 weights describing the maximal number of times each variable is evaluated in a program evaluation starting with an initial state. 
 This weight comes from the \emph{dependency quantity} from the second step analysis results.
 \\
 The adaptivity is then defined as the length of the walk visiting most query-related variables on this graph. 
 \end{enumerate}
 % \item 
\subsubsection{Static Program Analysis}
\label{sec:adapt-intro-static}
% \paragraph{Limitations of Existing Execution-Based Program Analysis Techniques}
%
Though we have the intuitive \emph{Adaptivity} formally and precisely defined through the execution-based program analysis,
it is still not useful enough for programmers.
The weakness of the execution-based formalization comes from its in-efficiency.
It can only give the \emph{Adaptivity} number
after the program is executed multiple times.
When the query requests, the multiple executions consume a large amount of computation and communication resources.
This requires us to develop an efficient analysis method, 
which can provide the programmers with the \emph{Adaptivity} quantity.
Because the static program analysis techniques usually don't require the execution of the program,
they are more efficient than the execution-based formalization.
Motivated by this, we develop a static program analysis framework, named {\THESYSTEM}.
% the through static-based 
% program analysis techniques.
% % In order to provide the programmers with useful information in an efficient way,
% I develop a static program analysis
% to estimate this adaptivity quantity efficiently.
% % , also without lose the usefulness of the information. 
% Through
\paragraph{Research Goal - Adaptivity Estimation}
Targeting on the third challenge
% --\textbf{Adaptivity Formalization}--
for estimating the adaptivity, 
the major research goal is to develop analysis method which can
% The second challenge is 
% give a sound and accurate \emph{upper bound} for the 
provide the programmers with the \emph{adaptivity} quantity
soundly, accurately and
efficiently
% defined 
through static program analysis.
% for a given data analysis program.
% Intuitively, a query $Q$ may depend on another query $P$, if there are two values that $P$ can return which affect in different ways the execution of $Q$. 
% For example, as shown in \cite{dwork2015reusable}, and as I did in our example in Figure~\ref{fig:generalization_errors}(a), one can design a machine learning algorithm for constructing a classifier that first computes each feature's correlations with the label via a sequence of queries, and then constructs the classifier based on the correlation values. 
% If one feature's correlation changes, the classifier depending on features is also affected. 
% The adaptive data analysis model I consider and our definition of adaptivity suggest that for this task I can use a program analysis that is based on some form of dependency analysis. This analysis needs to take into consideration:
There are three sub-research goals in estimating \emph{adaptivity} statically.
Each goal aims to resolve a fact introduced in the \textbf{Adaptivity Estimation} challenge in Section~\ref{sec:adapt-motivation}.
\\
subgoal-1) Designing a data flow analysis algorithm, aims to estimate whether a query may depend on the other queries. 
This analysis needs to consider the monolithic blocks of the program, through the use of variables and values, on other parts of the program.
% the fact that, in general, a query $Q$ is not a monolithic block but rather it may depend, 
% through the use of variables and values, on other parts of the program. 
% Hence, it needs to consider some form of data flow analysis. 
\\
subgoal-2) Integrating the control flow analysis techniques into the static analysis,
 aims to 
capture the query dependency soundly
% in the case
% fact that, in general, the decision on 
% where the dependent query might not be executed under the 
under the control influence.
% of the first query. 
% Hence, 
% it needs to consider some form of control flow analysis.
 \\
subgoal-3) Combining the reachability bound analysis techniques into the static analysis,
% I'm not only interested in whether there is a dependency or not, but in the length of the chain of dependencies. 
% Hence, it needs to consider some 
aims to estimate
% the quantitative information on the query requests dependencies. 
the dependency depth for query requests.
% {A quick example is that: I store the result of query $Q_1$ in variable $x$ and use variable $y$ to record the result of query $Q_2$. I want to construct the third query $Q_3$ which relies on the value stored in $x$, let us say, $Q_3$ will ask for the sum of the first column of a table if $x$ is positive and the sum of the second column otherwise. In this situation, I need data flow analysis. On the other hand, if I need the value of $y$ to help us decide whether I should ask $Q_3$, for example, I ask the third query if $y$ is odd, and do not ask if $y$ is even. Naturally, to be able to handle this case, control flow analysis comes into play. Formally speaking, }
\paragraph{Limitations of Existing Works}
\begin{itemize}
 \item \textbf{Limitations in Static Program Analysis Area}
 \\
 There are many static program analysis techniques, which are widely used in dependency analysis. 
 However, it is not straightforward when applied to adaptivity analysis.
 \begin{enumerate}
 \item To the best of my knowledge,
 existing static dependency analysis techniques do not consider both control influence and value influence.
 But in order to analyze and estimate the \emph{adaptivity} statically, as discussed above, we need to consider both of them.
 \item The existing static analysis techniques in the complexity or resource cost estimation areas,
 cannot give accurate reachability times on programs 
 at every execution location.
 \item Moreover, existing static analysis techniques and their research focus consider
 either only the dependency analysis
 or only the program quantitative property (such as complexity or resource cost),
 but not both of them.
 % No need to mention the adaptivity analysis.
 \end{enumerate}
\item \textbf{Limitations in Previous Works on Estimating Adaptivity}
The previous works from \cite{weihao22}
% ?on estimating this quantity from \cite{weihao22} are limited in the expressiveness, efficiency, and accuracy as well.
% In the previous 
develop program analysis algorithm for estimating the adaptivity.
However, this algorithm is low efficiency, and in-precise. It is not automatic either. 
% The \emph{adaptivity} quantities in many general data analysis programs are unable to be defined in the previous \emph{adaptivity} definition.
% For the program which is able to be defined,
% they cannot give the most precise adaptivity definitions matching the intuitive \emph{adaptivity}.
This analysis algorithm and limitations are introduced with more details in Section~\ref{sec:prework-formalization}.
\end{itemize}

\paragraph{Methodology}
To address these considerations and be able to estimate a sound upper bound on the adaptivity of a program, 
I develop a static program adaptivity analysis framework, named {\THESYSTEM} in Chapter~\ref{sec:adapt-static}.
% which 
% {\THESYSTEM} combines data flow and control flow analysis with reachability bound analysis~\cite{GulwaniZ10}. 
% This new program analysis gives tighter bounds on the adaptivity of a program than the ones one would achieve by directly using the data and control flow analyses or the ones that one would achieve by directly using reachability bound analysis techniques alone. Specifically as follows in the same 3 aspects as the execution-based analysis 
% while through static program analysis techniques, a sound estimated result will be given in each aspect as follows.
This analysis combines data flow and control flow analysis with reachability bound analysis.
% ~\cite{GulwaniZ10}. 
This new program analysis gives tighter bounds on the adaptivity of a program than the ones one would achieve 
by directly using the data and control flow analyses or the ones that one would achieve 
by directly using reachability bound analysis techniques alone. 
% Specifically as follows in the same 
It is developed in 3 aspects similar to the execution-based adaptivity analysis 
while through static program analysis techniques. 
A sound estimated result is given in each part, which is summarized as follows.
\begin{enumerate}
% \item The data dependency relation analysis through the static data flow analysis technique.
% \item The dependency quantity analysis through the static program reachability bound analysis techniques.
% \item The program adaptivity estimation, through newly designed algorithms based on the results estimated above, 
% computing the adaptivity upper bound soundly 
% and accurately.
\item The {\THESYSTEM} analyzes the data \emph{dependency relation} through the static data flow analysis technique in Section~\ref{sec:static-dep}.
This analysis corresponds to the first step in execution-based adaptivity analysis. 
The estimated result produced from 
this step is proved as a sound upper bound for the \emph{dependency relation} defined in the adaptivity formalization model from Section~\ref{sec:dynamic-datadep}.
\item 
% Still , 
Corresponding to the second step in execution-based adaptivity analysis, the \emph{dependency quantity} 
is estimated by {\THESYSTEM} through the static program reachability bound analysis techniques, in Section~\ref{sec:static-quantity}.
% This analysis corresponds to the second step in execution-based adaptivity analysis. 
The estimated result produced from 
this step is proved as a sound upper bound for the \emph{dependency quantity} defined in the adaptivity formalization model.
\item 
% The program estimation, 
% In 
The static program adaptivity analysis in this step
% , specifically estimating 
estimates the \emph{adaptivity} formalized in the third step of adaptivity formalization in Section~\ref{sec:static-adapt}.
% is presented in Section~\ref{sec:static-reachability}.
% the program adaptivity estimation, 
According to the third step of execution-based adaptivity analysis, 
{\THESYSTEM} in this step also constructs a program-based dependence graph for approximating the execution-based dependency graph.
% in Section~\ref{sec:dynamic-adapt}.
Then, based on this graph, {\THESYSTEM} 
% I design an algorithm
% based on the results estimated above, 
% computing 
computes the adaptivity upper bound soundly 
and accurately through a newly designed algorithm.
\end{enumerate}