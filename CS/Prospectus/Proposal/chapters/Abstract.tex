Data analyses are usually designed to identify some property of the population from which the data are drawn, generalizing beyond the specific data sample. For this reason, data analyses are often designed in a way that guarantees that they produce a low generalization error.
   That is, they are designed so that the result of a data analysis run on a sample data does not differ too much from the result one would achieve by running the analysis over the entire population. 
   
   An adaptive data analysis can be seen as a process composed by multiple queries interrogating some data, where the choice of which query to run next may rely on the results of previous queries. 
   The generalization error of each individual query/analysis can be controlled by using an array of well-established statistical techniques.
   However, when queries are arbitrarily composed, the different errors can propagate through the chain of different queries and bring to high generalization error. 
   To address this issue, data analysts are designing several techniques that not only guarantee bounds on the generalization errors of single queries, but that also guarantee bounds on the generalization error of the composed analyses. 
   The choice of which of these techniques to use, often depends on the chain of queries that an adaptive data analysis can generate.
   %The total number of queries and the depth of the chain of queries are of great significance to guarantee the generalization error, when the composed data analyses are adaptive. 
   % Gap
   % Unfortunately, this depth which relies on the program(implementation) itself is costly in human efforts, and how to statically obtain this information is not well studied to support data analysts.
   
   In this proposal, I will focus on analyse this intuitive \emph{adaptivity} property for the adaptive data analysis programs. 
   Then I will extend this analysis with improved techniques and generalize this analysis onto program's resource cost analysis.
   \\
   Firstly I will analyse this \emph{adaptivity} property for the adaptive data analysis programs in a while-like language.
   Through two aspects: the execution-based analysis and static-based program analysis.
	In the execution-based analysis, I will formalize the intuitive notion of \emph{adaptivity} as a quantitative 
   property of programs. This analysis is developed in three steps through different methodologies in each step. 
   \\
	a. The dependency relation between every query, through the methodology of semantic data dependency analysis.
   \\
	b. The dependency quantity analysis, through the methodology of execution-based data reachability bound analysis.
   \\
	c. The adaptivity analysis, based on the two analysis results above, give the formal \emph{adaptivity} model 
   for program.
   \\   
   % I will focus on research on how to define the Adaptivity semantically. 
   % (the Trace, Event, the Dependency relation, Dependency depth in terms of the evaluation times and the Adaptivity)
	In the static-based program analysis, I will design a static program analysis for soundly approximating this quantity.
   In this static program analysis, the program will be analysed in the same 3 aspects as the execution-based analysis 
   while through static program analysis techniques, and a sound estimated result will be given in each aspect.
   \\
	a. The data dependency relation analysis through the static data flow analysis technique.
   \\
	b. The dependency quantity analysis through the static program reachability bound analysis techniques.
   \\
	c. Estimation, through algorithm designed based on the two results above, computing the adaptivity upper bound soundly 
   and accurately.
   \\
   I will implement my program analysis and show that it can help to analyse the adaptivity of several concrete data analyses with different adaptivity structures.

   Then, based on the implementation and experimental results, 
   I will focus on improve three features of this full-spectrum analysis.
   \\
   1. I will improve the precision of the \emph{adaptivity} in the formalized model through the execution-based program analysis.
   \\
   2. In static program analysis, I will give tighter estimated upper bound on dependency quantity through 
   path sensitive reachability bound analysis techniques. 
   \\
   3. In the third step of static program analysis, I will improve the accuracy of the adaptivity computation algorithm,
   compute a tighter adaptivity upper bound as well.
   
   Finally, through observations that the heap resource consumption during the program 
   execution increases and particularly decreases implicitly in the same way as the program's adaptivity, 
   % Specifically, in line 5 
   % where the list is re-written and the heap consumption is decreased implicitly. 
   % This implicit decrease 
   % of the cost works exactly the same as program's adaptivity decrease.
   I'm interested in generalizing this quantity analysis framework onto the program's resource cost analysis. Use this framework,
   I will give
   a more accurate resource cost estimation by taking the program's implicit resource cost into consideration, comparing 
   to the worst case cost analysis in traditional way.
